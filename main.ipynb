{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the [IMDb movie reviews sentiment analysis dataset](https://ai.stanford.edu/%7Eamaas/data/sentiment/) which contains **movie reviews** posted by people on the [IMDb website](https://www.imdb.com/), as well as the corresponding labels (\"positive” or “negative”) indicating whether the reviewer liked the movie or not. There are 50,000 movie reviews divided into 25,000 reviews for training and 25,000 reviews for testing. The training and test sets are balanced, meaning they contain the same number of positive and negative reviews.\n",
    "\n",
    "The data samples may be in a specific order. A simple best practice to ensure the model is not affected by data order is to always first shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path, seed=123):\n",
    "\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    Arguments\n",
    "    - data_path: string, path to the data directory.\n",
    "    - seed: int, seed for randomizer.\n",
    "\n",
    "    Returns\n",
    "    - A tuple of training and validation data.\n",
    "    - Number of training samples: 25000\n",
    "    - Number of test samples: 25000\n",
    "    - Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    References\n",
    "    - Download and uncompress archive from:\n",
    "    http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname), encoding='utf-8') as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname), encoding='utf-8') as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(r\"C:\\Users\\danie\\Desktop\\Projects\\sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 25000\n",
      "Number of test samples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_data[0])}\")\n",
    "print(f\"Number of test samples: {len(test_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our data looks like and check if the sentiment label corresponds to the sentiment of the review in a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pointless movie with nothing but gratuitous violence. The only fun I had was playing \"spot the location\", as much of it was filmed in my home town of Regina, Saskatchewan. I like to support locally produced films but this one was a major disappointment.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected sentiment (negative) matches the sample’s label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chose a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google Developers](https://developers.google.com/machine-learning/guides/text-classification/step-2-5) has created the following model selection algorithm and flowchart attempting to significantly simplify the process of selecting a text classification model. For a given dataset, our goal is to find the algorithm that achieves close to maximum accuracy while minimizing computation time required for training.\n",
    "\n",
    "**Model selection algorithm:**\n",
    "1. Calculate the *number of samples* to *number of words per sample* ratio.\n",
    "2. If this ratio is less than 1500, tokenize the text as n-grams and use a simple multi-layer perceptron (MLP) model to classify them (left branch in the flowchart below):\n",
    "- Split the samples into word n-grams and convert the n-grams into vectors.\n",
    "- Score the importance of the vectors and then select the top 20K using the scores.\n",
    "- Build an MLP model.\n",
    "3. If the ratio is greater than 1500, tokenize the text as sequences and use a sepCNN model to classify them (right branch in the flowchart below):\n",
    "- Split the samples into words and select the top 20K words based on their frequency.\n",
    "- Convert the samples into word sequence vectors.\n",
    "- If the ratio is less than 15K, using a fine-tuned pre-trained embedding with the sepCNN model will likely provide the best results.\n",
    "4. Measure the model performance with different hyperparameter values to find the best model configuration for the dataset.\n",
    "\n",
    "**Flowchart:**\n",
    "\n",
    "In the flowchart below, the yellow boxes indicate data and model preparation processes. Grey boxes and green boxes indicate choices they considered for each process. Green boxes indicate their recommended choice for each process.\n",
    "\n",
    "<div style=\"width: 700px; overflow: hidden;\">\n",
    "    <img src=\"https://developers.google.com/static/machine-learning/guides/text-classification/images/TextClassificationFlowchart.png\" width=\"100%\" alt=\"Your Image\">\n",
    "</div>\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "Models can be broadly classified into two categories:\n",
    "\n",
    "- **n-gram models:** Models that just see text as “bags” (sets) of words. Types of n-gram models include logistic regression, simple multi-layer perceptrons (MLPs, or fully-connected neural networks), gradient boosted trees and support vector machines.\n",
    "\n",
    "- **Sequence models:** Models that use word ordering information. Types of sequence models include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and their variations. \n",
    "\n",
    "\n",
    "The ratio of *number of samples* to *number of words per sample* correlates with which model performs well. \n",
    "- When the value for this ratio is **small (<1500)**, small multi-layer perceptrons that take **n-grams** as input perform better or at least as well as sequence models. MLPs are simple to define and understand, and they take much less compute time than sequence models. \n",
    "- When the value for this ratio is **large (>= 1500)**, a **sequence** model is a better option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Gets the median number of words per sample given corpus.\n",
    "\n",
    "    Arguments\n",
    "    - sample_texts: list, sample texts.\n",
    "\n",
    "    Returns\n",
    "    - int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_words_per_sample(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our IMDb review dataset, the ratio of *number of samples* to *number of words per sample* is less than 1500 so we should choose a n-gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-procesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms take numbers as inputs. This means that we will need to convert the texts into numerical vectors. There are two steps to this process:\n",
    "\n",
    "- **Tokenization:** Divide the texts into words or smaller sub-texts, which will enable good generalization of relationship between the texts and the labels. This determines the “vocabulary” of the dataset (set of unique tokens present in the data).\n",
    "\n",
    "Vectorization: Define a good numerical measure to characterize these texts.\n",
    "\n",
    "Let’s see how to perform these two steps for both n-gram vectors and sequence vectors, as well as how to optimize the vector representations using feature selection and normalization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Although it\\'s most certainly politically incorrect to be entertained by a drunk, there\\'s such a charm to Dudley Moore\\'s portrayal of lovable lush, Arthur Bach one can\\'t help but feel for this unique and wonderful character. How can you not be entertained by that infectious laugh and giggle and utter silliness. Although I\\'m not really a Liza Minnelli fan, she was really excellent as Linda Marolla and I couldn\\'t picture anyone else in that role. Sir John Gielgud was the heart of the film and deserved his Oscar. The rest of the cast also excellent and that great tune \"Arthur\\'s Theme\", wow. Truly this was one of the Best Comedies of the 1980s. Great films get better with each viewing and that is the case with \"Arthur.\"'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

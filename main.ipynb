{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the [IMDb movie reviews sentiment analysis dataset](https://ai.stanford.edu/%7Eamaas/data/sentiment/) which contains **movie reviews** posted by people on the [IMDb website](https://www.imdb.com/), as well as the corresponding labels (\"positive” or “negative”) indicating whether the reviewer liked the movie or not. There are 50,000 movie reviews divided into 25,000 reviews for training and 25,000 reviews for testing. The training and test sets are balanced, meaning they contain the same number of positive and negative reviews.\n",
    "\n",
    "The data samples may be in a specific order. A simple best practice to ensure the model is not affected by data order is to always first shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path, seed=123):\n",
    "\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    Arguments\n",
    "    - data_path: string, path to the data directory.\n",
    "    - seed: int, seed for randomizer.\n",
    "\n",
    "    Returns\n",
    "    - A tuple of training and validation data.\n",
    "    - Number of training samples: 25000\n",
    "    - Number of test samples: 25000\n",
    "    - Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    References\n",
    "    - Download and uncompress archive from:\n",
    "    http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname), encoding='utf-8') as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname), encoding='utf-8') as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(r\"C:\\Users\\danie\\Desktop\\Projects\\sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 25000\n",
      "Number of test samples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_data[0])}\")\n",
    "print(f\"Number of test samples: {len(test_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our data looks like and check if the sentiment label corresponds to the sentiment of the review in a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pointless movie with nothing but gratuitous violence. The only fun I had was playing \"spot the location\", as much of it was filmed in my home town of Regina, Saskatchewan. I like to support locally produced films but this one was a major disappointment.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected sentiment (negative) matches the sample’s label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chose a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google Developers](https://developers.google.com/machine-learning/guides/text-classification/step-2-5) have created the following model selection algorithm and flowchart attempting to significantly simplify the process of selecting a text classification model. For a given dataset, our goal is to find the algorithm that achieves close to maximum accuracy while minimizing computation time required for training.\n",
    "\n",
    "Models can be broadly classified into two categories:\n",
    "\n",
    "- **N-gram models (Models that just see text as “bags” (sets) of words):** With n-gram vector representation, we discard a lot of information about word order and grammar. This representation is used in conjunction with models that don’t take ordering into account, such as logistic regression, simple multi-layer perceptrons (MLPs, or fully-connected neural networks), gradient boosted trees and support vector machines.\n",
    "\n",
    "- **Sequence models (Models that use word ordering information):** For some text samples, word order is critical to the text’s meaning. Models such as convolutional neural networks (CNNs), and recurrent neural networks (RNNs) can infer meaning from the order of words in a sample.\n",
    "\n",
    "\n",
    "The ratio of *number of samples* to *number of words per sample* correlates with which model performs better. \n",
    "- When the value for this ratio is **small (<1500)**, small multi-layer perceptrons that take **n-grams** as input perform better or at least as well as sequence models. MLPs are simple to define and understand, and they take much less compute time than sequence models. \n",
    "- When the value for this ratio is **large (>= 1500)**, a **sequence** model is a better option.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Model selection algorithm:**\n",
    "1. Calculate the *number of samples* to *number of words per sample* ratio.\n",
    "2. If this ratio is less than 1500, tokenize the text as n-grams and use a simple multi-layer perceptron (MLP) model to classify them (left branch in the flowchart below):\n",
    "- Split the samples into word n-grams and convert the n-grams into vectors.\n",
    "- Score the importance of the vectors and then select the top 20K using the scores.\n",
    "- Build an MLP model.\n",
    "3. If the ratio is greater than 1500, tokenize the text as sequences and use a sepCNN model to classify them (right branch in the flowchart below):\n",
    "- Split the samples into words and select the top 20K words based on their frequency.\n",
    "- Convert the samples into word sequence vectors.\n",
    "- If the ratio is less than 15K, using a fine-tuned pre-trained embedding with the sepCNN model will likely provide the best results.\n",
    "4. Measure the model performance with different hyperparameter values to find the best model configuration for the dataset.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Flowchart:**\n",
    "\n",
    "In the flowchart below, the yellow boxes indicate data and model preparation processes. Grey boxes and green boxes indicate choices they considered for each process. Green boxes indicate their recommended choice for each process.\n",
    "\n",
    "<div style=\"width: 700px; overflow: hidden;\">\n",
    "    <img src=\"https://developers.google.com/static/machine-learning/guides/text-classification/images/TextClassificationFlowchart.png\" width=\"100%\" alt=\"Your Image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the *number of samples* to *number of words per sample* ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Gets the median number of words per sample given corpus.\n",
    "\n",
    "    Arguments\n",
    "    - sample_texts: list, sample texts.\n",
    "\n",
    "    Returns\n",
    "    - int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143.67816091954023"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25000 / get_num_words_per_sample(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our IMDb review dataset, the ratio of *number of samples* to *number of words per sample* is less than 1500 so we should choose a n-gram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-procesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms take numbers as inputs. This means that we will need to convert the texts into numerical vectors. There are two steps to this process:\n",
    "\n",
    "- **1. Tokenization:** Divide the texts into words or smaller sub-texts, which will enable good generalization of relationship between the texts and the labels. This determines the “vocabulary” of the dataset (set of unique tokens present in the data).\n",
    "- **2. Vectorization:** Define a good numerical measure to characterize these texts.\n",
    "\n",
    "Let’s see how to perform these two steps for both **n-gram vectors** and **sequence vectors**, as well as how to optimize the vector representations using feature selection and normalization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an n-gram vector, text is represented as a **collection of unique n-grams: groups of n adjacent tokens** (typically, words).\n",
    "\n",
    "<br>\n",
    "\n",
    "**1. Tokenization**\n",
    "\n",
    "First, we have to split (tokenize) the text samles into word unigrams and bigrams. Thus, we will determines the \"vocabulary\" of the dataset. In the case of the text sample *'The mouse ran up the clock'*.\n",
    "\n",
    "- The word unigrams (n = 1) are ['the', 'mouse', 'ran', 'up', 'clock']\n",
    "\n",
    "- The word bigrams (n = 2) are ['the mouse', 'mouse ran', 'ran up', 'up the', 'the clock']\n",
    "\n",
    "<br>\n",
    "\n",
    "**2. Vectorization**\n",
    "\n",
    "Once we have split our text samples into n-grams, we need to turn these n-grams into numerical vectors that our machine learning models can process. In the case of the text samples *'The mouse ran up the clock'* and *'The mouse ran down'*.\n",
    "\n",
    "- The indexes assigned to the unigrams and bigrams would be {'clock': 0, 'down': 1, 'mouse': 2, 'mouse ran': 3, 'ran': 4, 'ran down': 5, 'ran up': 6, 'the': 7, 'the clock': 8, 'the mouse': 9, 'up': 10, 'up the': 11}\n",
    "\n",
    "\n",
    "Once indexes are assigned to the n-grams, we typically vectorize the text samples using one-hot encoding, count encoding, or **Tf-idf encoding**. This last option is recommend for vectorizing n-grams. In the case of the  text sample *'The mouse ran up the clock'*.\n",
    "- The vectorization using Tf-idf encoding would be [0.33, 0, 0.23, 0.23, 0.23, 0, 0.33, 0.47, 0.33, 0.23, 0.33, 0.33]\n",
    "\n",
    "<br>\n",
    "\n",
    "**3. Feature selection**\n",
    "\n",
    "When we convert all of the texts in a dataset into word uni+bigram tokens, we may end up with tens of thousands of tokens. Not all of these tokens/features contribute to label prediction so we can drop certain tokens, for instance those that occur extremely rarely across the dataset. We can also measure feature importance (how much each token contributes to label predictions), and only include the most informative tokens. Two commonly used functions to calculate feature importance are **f_classif** and chi2. In addition, it has been noticed that accuracy peaks at around 20,000 features for many datasets.\n",
    "\n",
    "<br>\n",
    "\n",
    "The following code:\n",
    "- Tokenize text samples into word unigrams + bigrams.\n",
    "- Vectorize using tf-idf encoding.\n",
    "- Select only the top 20,000 features from the vector of tokens by discarding tokens that appear fewer than 2 times and using f_classif to calculate feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range (inclusive) of n-gram sizes for tokenizing text\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Whether text should be split into word or character n-grams ('word' or 'char')\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features\n",
    "TOP_K = 20000\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded\n",
    "MIN_DOCUMENT_FREQUENCY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors:\n",
    "    \n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams.\n",
    "\n",
    "    Arguments:\n",
    "    - train_texts: list, training text strings.\n",
    "    - train_labels: np.ndarray, training labels.\n",
    "    - val_texts: list, validation text strings.\n",
    "\n",
    "    Returns:\n",
    "    - x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "        'ngram_range': NGRAM_RANGE,\n",
    "        'dtype': 'int32',\n",
    "        'strip_accents': 'unicode',\n",
    "        'decode_error': 'replace',\n",
    "        'analyzer': TOKEN_MODE,\n",
    "        'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sequence vector, text is represented as a **sequence of tokens, preserving order**.\n",
    "\n",
    "<br>\n",
    "\n",
    "**1. Tokenization**\n",
    "\n",
    "Text can be represented as either a sequence of characters, or a sequence of words. Using word-level representation provides better performance than character tokens. Using character tokens makes sense only if texts have lots of typos.\n",
    "\n",
    "<br>\n",
    "\n",
    "**2. Vectorization**\n",
    "\n",
    "Once we have converted our text samples into sequences of words, we need to turn these sequences into numerical vectors. The example below shows the indexes assigned to the unigrams generated for two texts, and then the sequence of token indexes to which the first text is converted.\n",
    "\n",
    "- Texts: 'The mouse ran up the clock' and 'The mouse ran down'\n",
    "- Index assigned for every token: {'the': 1, 'mouse': 2, 'ran': 3, 'up': 4,'clock': 5, 'down': 6}.\n",
    "- Sequence of token indexes: 'The mouse ran up the clock' = [1, 2, 3, 4, 1, 5]\n",
    "\n",
    "Note that 'the' occurs most frequently, so the index value of 1 is assigned to it. Also some libraries reserve index 0 for unknown tokens, as is the case here.\n",
    "\n",
    "To vectorize the token sequences we can use one-hot encoding, or **word embeddings**. This last option is recommend for vectorizing sequences since words have meaning(s) associated with them. As a result, we can represent word tokens in a dense vector space (~few hundred real numbers), where the location and distance between words indicates how similar they are semantically.\n",
    "\n",
    "\n",
    "<div style=\"width: 1200px; overflow: hidden;\">\n",
    "    <img src=\"https://developers.google.com/static/machine-learning/guides/text-classification/images/WordEmbeddings.png\" width=\"100%\" alt=\"Your Image\">\n",
    "</div>\n",
    "\n",
    "Sequence models often have such an embedding layer as their first layer. This layer learns to turn word index sequences into word embedding vectors during the training process, such that each word index gets mapped to a dense vector of real values representing that word’s location in semantic space.\n",
    "\n",
    "<div style=\"width: 1200px; overflow: hidden;\">\n",
    "    <img src=\"https://developers.google.com/static/machine-learning/guides/text-classification/images/EmbeddingLayer.png\" width=\"100%\" alt=\"Your Image\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing import text\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.python.keras.layers import Embedding\n",
    "from tensorflow.python.keras.layers import SeparableConv1D\n",
    "from tensorflow.python.keras.layers import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import GlobalAveragePooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the [IMDb movie reviews sentiment analysis dataset](https://ai.stanford.edu/%7Eamaas/data/sentiment/) which contains **movie reviews** posted by people on the [IMDb website](https://www.imdb.com/), as well as the corresponding labels (\"positive” or “negative”) indicating whether the reviewer liked the movie or not. There are 50,000 movie reviews divided into 25,000 reviews for training and 25,000 reviews for testing. The training and test sets are balanced, meaning they contain the same number of positive and negative reviews.\n",
    "\n",
    "The data samples may be in a specific order. A simple best practice to ensure the model is not affected by data order is to always first shuffle the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_path, seed=123):\n",
    "\n",
    "    \"\"\"Loads the IMDb movie reviews sentiment analysis dataset.\n",
    "\n",
    "    Arguments\n",
    "    - data_path: string, path to the data directory.\n",
    "    - seed: int, seed for randomizer.\n",
    "\n",
    "    Returns\n",
    "    - A tuple of training and validation data.\n",
    "    - Number of training samples: 25000\n",
    "    - Number of test samples: 25000\n",
    "    - Number of categories: 2 (0 - negative, 1 - positive)\n",
    "\n",
    "    References\n",
    "    - Download and uncompress archive from:\n",
    "    http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "    \"\"\"\n",
    "\n",
    "    imdb_data_path = os.path.join(data_path, 'aclImdb')\n",
    "\n",
    "    # Load the training data\n",
    "    train_texts = []\n",
    "    train_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        train_path = os.path.join(imdb_data_path, 'train', category)\n",
    "        for fname in sorted(os.listdir(train_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(train_path, fname), encoding='utf-8') as f:\n",
    "                    train_texts.append(f.read())\n",
    "                train_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Load the validation data\n",
    "    test_texts = []\n",
    "    test_labels = []\n",
    "    for category in ['pos', 'neg']:\n",
    "        test_path = os.path.join(imdb_data_path, 'test', category)\n",
    "        for fname in sorted(os.listdir(test_path)):\n",
    "            if fname.endswith('.txt'):\n",
    "                with open(os.path.join(test_path, fname), encoding='utf-8') as f:\n",
    "                    test_texts.append(f.read())\n",
    "                test_labels.append(0 if category == 'neg' else 1)\n",
    "\n",
    "    # Shuffle the training data and labels\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_texts)\n",
    "    random.seed(seed)\n",
    "    random.shuffle(train_labels)\n",
    "\n",
    "    return ((train_texts, np.array(train_labels)),\n",
    "            (test_texts, np.array(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = load_dataset(r\"C:\\Users\\danie\\Desktop\\Projects\\sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 25000\n",
      "Number of test samples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_data[0])}\")\n",
    "print(f\"Number of test samples: {len(test_data[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our data looks like and check if the sentiment label corresponds to the sentiment of the review in a random sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A pointless movie with nothing but gratuitous violence. The only fun I had was playing \"spot the location\", as much of it was filmed in my home town of Regina, Saskatchewan. I like to support locally produced films but this one was a major disappointment.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected sentiment (negative) matches the sample’s label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chose a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Google Developers](https://developers.google.com/machine-learning/guides/text-classification/step-2-5) have created the following model selection algorithm and flowchart attempting to significantly simplify the process of selecting a text classification model. For a given dataset, our goal is to find the algorithm that achieves close to maximum accuracy while minimizing computation time required for training.\n",
    "\n",
    "Models can be broadly classified into two categories:\n",
    "\n",
    "- **N-gram models (Models that just see text as “bags” (sets) of words):** With n-gram vector representation, we discard a lot of information about word order and grammar. This representation is used in conjunction with models that don’t take ordering into account, such as logistic regression, simple multi-layer perceptrons (MLPs, or fully-connected neural networks), gradient boosted trees and support vector machines.\n",
    "\n",
    "- **Sequence models (Models that use word ordering information):** For some text samples, word order is critical to the text’s meaning. Models such as convolutional neural networks (CNNs), and recurrent neural networks (RNNs) can infer meaning from the order of words in a sample.\n",
    "\n",
    "\n",
    "The ratio of *number of samples* to *number of words per sample* correlates with which model performs better. \n",
    "- When the value for this ratio is **small (<1500)**, small multi-layer perceptrons that take **n-grams** as input perform better or at least as well as sequence models. MLPs are simple to define and understand, and they take much less compute time than sequence models. \n",
    "- When the value for this ratio is **large (>= 1500)**, a **sequence** model is a better option. Sequence models are better when there are a large number of small, dense vectors. This is because embedding relationships are learned in dense space, and this happens best over many samples.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Model selection algorithm:**\n",
    "1. Calculate the *number of samples* to *number of words per sample* ratio.\n",
    "2. If this ratio is less than 1500, tokenize the text as n-grams and use a simple multi-layer perceptron (MLP) model to classify them (left branch in the flowchart below):\n",
    "- Split the samples into word n-grams and convert the n-grams into vectors.\n",
    "- Score the importance of the vectors and then select the top 20K using the scores.\n",
    "- Build an MLP model.\n",
    "3. If the ratio is greater than 1500, tokenize the text as sequences and use a sepCNN model to classify them (right branch in the flowchart below):\n",
    "- Split the samples into words and select the top 20K words based on their frequency.\n",
    "- Convert the samples into word sequence vectors.\n",
    "- If the ratio is less than 15K, using a fine-tuned pre-trained embedding with the sepCNN model will likely provide the best results.\n",
    "4. Measure the model performance with different hyperparameter values to find the best model configuration for the dataset.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Flowchart:**\n",
    "\n",
    "In the flowchart below, the yellow boxes indicate data and model preparation processes. Grey boxes and green boxes indicate choices they considered for each process. Green boxes indicate their recommended choice for each process.\n",
    "\n",
    "<div style=\"width: 700px; overflow: hidden;\">\n",
    "    <img src=\"https://developers.google.com/static/machine-learning/guides/text-classification/images/TextClassificationFlowchart.png\" width=\"100%\" alt=\"Your Image\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the *number of samples* to *number of words per sample* ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_words_per_sample(sample_texts):\n",
    "    \"\"\"Gets the median number of words per sample given corpus.\n",
    "\n",
    "    Arguments\n",
    "    - sample_texts: list, sample texts.\n",
    "\n",
    "    Returns\n",
    "    - int, median number of words per sample.\n",
    "    \"\"\"\n",
    "    num_words = [len(s.split()) for s in sample_texts]\n",
    "    return np.median(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143.67816091954023"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25000 / get_num_words_per_sample(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of our IMDb review dataset, the ratio of *number of samples* to *number of words per sample* is less than 1500 so we should choose a n-gram model. We will create a n-gran model and sequence model for practice and comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-procesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms take numbers as inputs. This means that we will need to convert the texts into numerical vectors. There are two steps to this process:\n",
    "\n",
    "- **1. Tokenization:** Divide the texts into words or smaller sub-texts, which will enable good generalization of relationship between the texts and the labels. This determines the “vocabulary” of the dataset (set of unique tokens present in the data).\n",
    "- **2. Vectorization:** Define a good numerical measure to characterize these texts.\n",
    "\n",
    "Let’s see how to perform these two steps for both **n-gram vectors** and **sequence vectors**, as well as how to optimize the vector representations using feature selection and normalization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an n-gram vector, text is represented as a **collection of unique n-grams: groups of n adjacent tokens** (typically, words).\n",
    "\n",
    "<br>\n",
    "\n",
    "**1. Tokenization**\n",
    "\n",
    "First, we have to split (tokenize) the text samles into word unigrams and bigrams. Thus, we will determines the \"vocabulary\" of the dataset. In the case of the text sample *'The mouse ran up the clock'*.\n",
    "\n",
    "- The word unigrams (n = 1) are ['the', 'mouse', 'ran', 'up', 'clock']\n",
    "\n",
    "- The word bigrams (n = 2) are ['the mouse', 'mouse ran', 'ran up', 'up the', 'the clock']\n",
    "\n",
    "<br>\n",
    "\n",
    "**2. Vectorization**\n",
    "\n",
    "Once we have split our text samples into n-grams, we need to turn these n-grams into numerical vectors that our machine learning models can process. In the case of the text samples *'The mouse ran up the clock'* and *'The mouse ran down'*.\n",
    "\n",
    "- The indexes assigned to the unigrams and bigrams would be {'clock': 0, 'down': 1, 'mouse': 2, 'mouse ran': 3, 'ran': 4, 'ran down': 5, 'ran up': 6, 'the': 7, 'the clock': 8, 'the mouse': 9, 'up': 10, 'up the': 11}\n",
    "\n",
    "\n",
    "Once indexes are assigned to the n-grams, we typically vectorize the text samples using one-hot encoding, count encoding, or **Tf-idf encoding**. This last option is recommend for vectorizing n-grams. In the case of the  text sample *'The mouse ran up the clock'*.\n",
    "- The vectorization using Tf-idf encoding would be [0.33, 0, 0.23, 0.23, 0.23, 0, 0.33, 0.47, 0.33, 0.23, 0.33, 0.33]\n",
    "\n",
    "<br>\n",
    "\n",
    "**3. Feature selection**\n",
    "\n",
    "When we convert all of the texts in a dataset into word uni+bigram tokens, we may end up with tens of thousands of tokens. Not all of these tokens/features contribute to label prediction so we can drop certain tokens, for instance those that occur extremely rarely across the dataset. We can also measure feature importance (how much each token contributes to label predictions), and only include the most informative tokens. Two commonly used functions to calculate feature importance are **f_classif** and chi2. In addition, it has been noticed that accuracy peaks at around 20,000 features for many datasets.\n",
    "\n",
    "<br>\n",
    "\n",
    "The following code:\n",
    "- Tokenize text samples into word unigrams + bigrams.\n",
    "- Vectorize using tf-idf encoding.\n",
    "- Select only the top 20,000 features from the vector of tokens by discarding tokens that appear fewer than 2 times and using f_classif to calculate feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range (inclusive) of n-gram sizes for tokenizing text\n",
    "NGRAM_RANGE = (1, 2)\n",
    "\n",
    "# Whether text should be split into word or character n-grams ('word' or 'char')\n",
    "TOKEN_MODE = 'word'\n",
    "\n",
    "# Limit on the number of features. We use the top 20K features\n",
    "TOP_K = 20000\n",
    "\n",
    "# Minimum document/corpus frequency below which a token will be discarded\n",
    "MIN_DOCUMENT_FREQUENCY = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_vectorize(train_texts, train_labels, val_texts):\n",
    "    \"\"\"Vectorizes texts as n-gram vectors\n",
    "    \n",
    "    1 text = 1 tf-idf vector the length of vocabulary of unigrams + bigrams\n",
    "\n",
    "    Arguments:\n",
    "    - train_texts: list, training text strings\n",
    "    - train_labels: np.ndarray, training labels\n",
    "    - val_texts: list, validation text strings\n",
    "\n",
    "    Returns:\n",
    "    - x_train, x_val: vectorized training and validation texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Create keyword arguments to pass to the 'tf-idf' vectorizer.\n",
    "    kwargs = {\n",
    "        'ngram_range': NGRAM_RANGE,\n",
    "        'dtype': 'int32',\n",
    "        'strip_accents': 'unicode',\n",
    "        'decode_error': 'replace',\n",
    "        'analyzer': TOKEN_MODE,\n",
    "        'min_df': MIN_DOCUMENT_FREQUENCY,\n",
    "    }\n",
    "    vectorizer = TfidfVectorizer(**kwargs)\n",
    "\n",
    "    # Learn vocabulary from training texts and vectorize training texts.\n",
    "    x_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "    # Vectorize validation texts.\n",
    "    x_val = vectorizer.transform(val_texts)\n",
    "\n",
    "    # Select top 'k' of the vectorized features.\n",
    "    selector = SelectKBest(f_classif, k=min(TOP_K, x_train.shape[1]))\n",
    "    selector.fit(x_train, train_labels)\n",
    "    x_train = selector.transform(x_train).astype('float32')\n",
    "    x_val = selector.transform(x_val).astype('float32')\n",
    "    return x_train, x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a sequence vector, text is represented as a **sequence of tokens, preserving order**.\n",
    "\n",
    "<br>\n",
    "\n",
    "**1. Tokenization**\n",
    "\n",
    "Text can be represented as either a sequence of characters, or a sequence of words. Using word-level representation provides better performance than character tokens. Using character tokens makes sense only if texts have lots of typos.\n",
    "\n",
    "<br>\n",
    "\n",
    "**2. Vectorization**\n",
    "\n",
    "Once we have converted our text samples into sequences of words, we need to turn these sequences into numerical vectors. The example below shows the indexes assigned to the unigrams generated for two texts, and then the sequence of token indexes to which the first text is converted.\n",
    "\n",
    "- Texts: 'The mouse ran up the clock' and 'The mouse ran down'\n",
    "- Index assigned for every token: {'the': 1, 'mouse': 2, 'ran': 3, 'up': 4,'clock': 5, 'down': 6}.\n",
    "- Sequence of token indexes: 'The mouse ran up the clock' = [1, 2, 3, 4, 1, 5]\n",
    "\n",
    "Note that 'the' occurs most frequently, so the index value of 1 is assigned to it. Also some libraries reserve index 0 for unknown tokens, as is the case here.\n",
    "\n",
    "To vectorize the token sequences we can use one-hot encoding, or **word embeddings**. This last option is recommend for vectorizing sequences since words have meaning(s) associated with them. As a result, we can represent word tokens in a dense vector space (~few hundred real numbers), where the location and distance between words indicates how similar they are semantically.\n",
    "\n",
    "\n",
    "<div style=\"width: 900px; overflow: hidden;\">\n",
    "    <img src=\"https://developers.google.com/static/machine-learning/guides/text-classification/images/WordEmbeddings.png\" width=\"100%\" alt=\"Your Image\">\n",
    "</div>\n",
    "\n",
    "Sequence models often have such an embedding layer as their first layer. This layer learns to **turn word index sequences into word embedding vectors** during the training process, such that **each word index gets mapped to a dense vector** of real values representing that word’s location in semantic space.\n",
    "\n",
    "<div style=\"width: 900px; overflow: hidden;\">\n",
    "    <img src=\"https://developers.google.com/static/machine-learning/guides/text-classification/images/EmbeddingLayer.png\" width=\"100%\" alt=\"Your Image\">\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "**3. Feature selection**\n",
    "\n",
    "As in n-gram models, not all words in our data contribute to label predictions. We can optimize our learning process by discarding rare or irrelevant words from our vocabulary. Again, using the most frequent 20,000 features is generally sufficient.\n",
    "\n",
    "<br>\n",
    "\n",
    "The following code:\n",
    "\n",
    "- Tokenizes the texts into words.\n",
    "- Creates a vocabulary using the top 20,000 tokens.\n",
    "- Converts the tokens into sequence vectors.\n",
    "- Pads the sequences to a fixed sequence length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit on the number of features. We use the top 20K features\n",
    "TOP_K = 20000\n",
    "\n",
    "# Limit on the length of text sequences (sequences longer than this will be truncated)\n",
    "MAX_SEQUENCE_LENGTH = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_vectorize(train_texts, val_texts):\n",
    "    \"\"\"Vectorizes texts as sequence vectors\n",
    "\n",
    "    1 text = 1 sequence vector with fixed length\n",
    "\n",
    "    Arguments\n",
    "    - train_texts: list, training text strings\n",
    "    - val_texts: list, validation text strings\n",
    "\n",
    "    # Returns\n",
    "    - x_train, x_val, word_index: vectorized training and validation texts and word index dictionary.\n",
    "    \"\"\"\n",
    "    # Create vocabulary with training texts.\n",
    "    tokenizer = text.Tokenizer(num_words=TOP_K)\n",
    "    tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "    # Vectorize training and validation texts.\n",
    "    x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "    x_val = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "    # Get max sequence length.\n",
    "    max_length = len(max(x_train, key=len))\n",
    "    if max_length > MAX_SEQUENCE_LENGTH:\n",
    "        max_length = MAX_SEQUENCE_LENGTH\n",
    "\n",
    "    # Fix sequence length to max value. Sequences shorter than the length are\n",
    "    # padded in the beginning and sequences longer are truncated at the beginning.\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
    "    return x_train, x_val, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there are **only 2 classes (binary classification)**, the model should **output a single probability score**. For instance, outputting 0.14 for a given input sample means “14% confidence that this sample is in class 1, 86% that it is in class 2.” To output such a probability score, the activation function of the last layer should be a sigmoid function, and the loss function used to train the model should be binary cross-entropy.\n",
    "\n",
    "When there are **more than 2 classes (multi-class classification)**, the model should **output one probability score per class**. The sum of these scores should be 1. For instance, outputting {0: 0.01, 1: 0.14, 2: 0.85} means “1% confidence that this sample is in class 1, 14% that it is in class 2, and 85% that it is in class 3.” To output these scores, the activation function of the last layer should be softmax, and the loss function used to train the model should be categorical cross-entropy.\n",
    "\n",
    "\n",
    "<div style=\"width: 900px; overflow: hidden;\">\n",
    "    <img src=\"https://developers.google.com/static/machine-learning/guides/text-classification/images/LastLayer.png\" width=\"100%\" alt=\"Your Image\">\n",
    "</div>\n",
    "\n",
    "The following code defines a function that takes the number of classes as input, and outputs the appropriate output layer **# of units** (1 unit for binary classification; otherwise 1 unit for each class) and **activation function**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_last_layer_units_and_activation(num_classes):\n",
    "    \"\"\"Gets the # units and activation function for the last network layer.\n",
    "\n",
    "    Arguments\n",
    "    - num_classes: int, number of classes.\n",
    "\n",
    "    Returns\n",
    "    - units, activation values.\n",
    "    \"\"\"\n",
    "    if num_classes == 2:\n",
    "        activation = 'sigmoid'\n",
    "        units = 1\n",
    "    else:\n",
    "        activation = 'softmax'\n",
    "        units = num_classes\n",
    "    return units, activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-layer perceptrons (MLPs)** typically perform better since they are simple to define and understand, provide good accuracy, and require relatively little computation. The following code defines a MLP model using tensorflow.keras, adding Dropout layers for regularization (to prevent overfitting to training samples):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    \"\"\"Creates an instance of a multi-layer perceptron model.\n",
    "\n",
    "    Arguments\n",
    "    - layers: int, number of `Dense` layers in the model.\n",
    "    - units: int, output dimension of the layers.\n",
    "    - dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "    - input_shape: tuple, shape of input to the model.\n",
    "    - num_classes: int, number of output classes.\n",
    "\n",
    "    Returns\n",
    "    - An MLP model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=op_units, activation=op_activation))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, sequence models that can **learn from the adjacency of tokens**. They generally have a larger number of parameters to learn. The first layer in these models is an **embedding layer**, which learns the relationship between the words in a dense vector space. Learning word relationships works best over many samples.\n",
    "\n",
    "Words in a given dataset are most likely not unique to that dataset. We can thus learn the relationship between the words in our dataset using other datasets. To do so, we can transfer an embedding learned from another dataset into our embedding layer. These embeddings are referred to as **pre-trained embeddings**. Using a pre-trained embedding gives the model a head start in the learning process.\n",
    "\n",
    "There are pre-trained embeddings available that have been trained using large corpora, such as [GloVe](https://nlp.stanford.edu/projects/glove/). GloVe embeddings trained on Wikipedia data may not align with the language patterns in our IMDb dataset. The relationships inferred may need some updating (the embedding weights may need contextual tuning). We can do this in two stages:\n",
    "\n",
    "- In the first run, with the embedding layer weights frozen, we allow the rest of the network to learn. At the end of this run, the model weights reach a state that is much better than their uninitialized values. For the second run, we allow the embedding layer to also learn, making fine adjustments to all weights in the network. We refer to this process as using a **fine-tuned embedding**.\n",
    "\n",
    "- Fine-tuned embeddings yield better accuracy. However, this comes at the expense of increased compute power required to train the network. Given a sufficient number of samples (the ratio of *number of samples* to *number of words per sample* > 15k), we could do just as well **learning an embedding from scratch**.\n",
    "\n",
    "**sepCNNs**, a convolutional network variant, is often more efficient and perform better than other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepcnn_model(blocks, filters, kernel_size, embedding_dim, dropout_rate, pool_size,\n",
    "                 input_shape, num_classes, num_features, use_pretrained_embedding=False,\n",
    "                 is_embedding_trainable=False, embedding_matrix=None):\n",
    "    \"\"\"Creates an instance of a separable CNN model.\n",
    "\n",
    "    Arguments\n",
    "    - blocks: int, number of pairs of sepCNN and pooling blocks in the model.\n",
    "    - filters: int, output dimension of the layers.\n",
    "    - kernel_size: int, length of the convolution window.\n",
    "    - embedding_dim: int, dimension of the embedding vectors.\n",
    "    - dropout_rate: float, percentage of input to drop at Dropout layers.\n",
    "    - pool_size: int, factor by which to downscale input at MaxPooling layer.\n",
    "    - input_shape: tuple, shape of input to the model.\n",
    "    - num_classes: int, number of output classes.\n",
    "    - num_features: int, number of words (embedding input dimension).\n",
    "    - use_pretrained_embedding: bool, true if pre-trained embedding is on.\n",
    "    - is_embedding_trainable: bool, true if embedding layer is trainable.\n",
    "    - embedding_matrix: dict, dictionary with embedding coefficients.\n",
    "\n",
    "    Returns\n",
    "    - A sepCNN model instance.\n",
    "    \"\"\"\n",
    "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Add embedding layer. If pre-trained embedding is used add weights to the\n",
    "    # embeddings layer and set trainable to input is_embedding_trainable flag.\n",
    "    if use_pretrained_embedding:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0],\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=is_embedding_trainable))\n",
    "    else:\n",
    "        model.add(Embedding(input_dim=num_features,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=input_shape[0]))\n",
    "\n",
    "    for _ in range(blocks-1):\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(SeparableConv1D(filters=filters,\n",
    "                                  kernel_size=kernel_size,\n",
    "                                  activation='relu',\n",
    "                                  bias_initializer='random_uniform',\n",
    "                                  depthwise_initializer='random_uniform',\n",
    "                                  padding='same'))\n",
    "        model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(SeparableConv1D(filters=filters * 2,\n",
    "                              kernel_size=kernel_size,\n",
    "                              activation='relu',\n",
    "                              bias_initializer='random_uniform',\n",
    "                              depthwise_initializer='random_uniform',\n",
    "                              padding='same'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(op_units, activation=op_activation))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
